Transformer 奠基
# Abstract
作者指出当前的序列转换模型大都基于复杂的RNN或者CNN, 使用encoder-decoder结构.当然还使用了注意力机制.
作者提出了一个更简单的架构: Transformer, 它只保留了注意力机制.
且无论在大量还是有限训练数据下,泛化性都极其优秀.
# Introduction
作者介绍了循环模型, 肯定了循环模型所带来的提升,但是强调了循环模型的限制仍然是存在的:
	循环模型是沿着输入序列进行因子计算的. 它会与步长对齐的生成一个隐藏状态 $h_t$, 它表示了从时刻0到当前时刻t所有输入的信息.有:
$$ h_t = F(h_{t-1}, x_t)$$ 
	循环模型自身的特性阻碍了训练样本间的平行化, 并且这种情况再更长的序列中会变得更严重.
注意力机制往往被用于序列任务, 但是除了少数情况, 其他主要被用于循环网络的连接.


# Conclusion
Transformer 是第一个仅仅完全基于注意力机制, 取代了使用encoder-decoder架构, 和多头注意力机制的RNN层.
对于翻译认为, 相比于RNN层, 它的训练更快. 在大赛上取得了sota, 甚至优于此前所有报道模型效果的集合.
作者计划将注意力模型应用在其他模态(后续有的VIT). 他们期望能够降低生成的顺序性.